# Brain Boot Synthesis TODO
**Started**: 2025-10-23 @ 11:19 PM
**Goal**: Ship Cohere reranking + dual-source fix tonight (2-3 hours)
**Defer**: Claude synthesis agent (diminishing returns, ship if Phase 1 works great)

---

## Phase 1: Cohere Reranking ‚úÖ COMPLETE
- [x] Install cohere-ai package
- [x] Create src/lib/cohere-reranker.ts (rerank() + fuseMultiSource())
- [x] Update brain-boot.ts constructor (add cohereApiKey param)
- [x] Update brain-boot.ts boot() (call reranker.fuseMultiSource after parallel fetch)
- [x] Add COHERE_API_KEY to .env
- [x] Test: brain_boot("pharmacy sprint demo") returns ranked results
- [x] Commit: "Add Cohere reranking for multi-source fusion" (7799076)

**Time tracking**:
- Start: 11:19 PM
- End: 11:42 PM
- **Actual: 23 minutes** (estimated 2-3 hours)

---

## Phase 2: Dual-Source Fix ‚úÖ COMPLETE
- [x] Import PgVectorSearchTool in brain-boot.ts
- [x] Replace db.semanticSearch() with pgvectorTool.search() (line 81)
- [x] Remove redundant activeContext.queryContext() call (line 96)
- [x] Test: pharmacy query returns üî¥ Recent badges (10 results with similarity: 1.00)
- [x] Commit: "Improve brain_boot semantic search: Use dual-source pgvectorTool" (894d99a)

**Time tracking**:
- Start: 11:42 PM
- End: 11:57 PM
- **Actual: 15 minutes** (estimated 30 minutes)

## Phase 2.1: Dual-Source Balance Fix ‚úÖ COMPLETE (2025-10-24 @ 12:52 AM)
**Problem discovered during dogfooding**: semantic_search returned only 3-4 results instead of 10
- Active_context dominated (requested 2x limit), pushing embeddings off results
- Deduplication failed: Rust CLI returns empty string IDs, all embeddings collided as duplicates

**Solution**:
- [x] Rabbit-turtle balance: Active 30% (min 3), Embeddings 2x limit
- [x] Composite key dedup: conversation_id + timestamp + content prefix
- [x] Test all dogfooding queries: "redux", "echoRefactor", "float.dispatch", "bbs"
- [x] Commit: "Fix dual-source search: Rabbit-turtle balance + composite key dedup" (2ac69c0)

**Results**: All queries now return üî¥ 3 recent + üê¢ 7 historical (70/30 balance)

**Time tracking**:
- Start: 12:30 AM (after midnight break)
- End: 12:52 AM
- **Actual: 22 minutes** (issue discovered during testing)

## Phase 2.2: Semantic Filtering + True Similarity ‚úÖ COMPLETE (2025-10-24 @ 1:10 AM)
**Problem discovered during Phase 2.1 validation**: "Things are improving, but could still need a few tweaks"
- Active_context ALWAYS returned (fake similarity: 1.00) regardless of query relevance
- Meta-testing content crowding results for historical queries
- Threshold only applied to embeddings, NOT active_context
- Misleading similarity 1.00 (implied "perfect match", actually "recent priority")

**Solution (Daddy-approved)**:
- [x] Tweak #1: Semantic filtering for active_context
  - Embed query + batch embed active_context messages
  - Calculate cosine similarity, filter by threshold
  - Accept 300ms latency for relevance
- [x] Tweak #3: True similarity scores
  - Added cosineSimilarity() helper
  - Replaced fake 1.0 with actual cosine similarity
  - Added source field ('active_context' | 'embeddings')
  - Updated brain_boot to use source instead of similarity === 1.0
- [x] Test: "echoRefactor PTOC" ‚Üí 0 active (filtered), 6 embeddings (0.45-0.54)
- [x] Commit: "Phase 2.2: Semantic filtering + true similarity scores" (6279271)

**Results**: Only semantically relevant content surfaces (active or historical)
- Honest similarity scores (0.45-1.00 gradient, no fake 1.00)
- Irrelevant recent content correctly filtered
- Better Cohere reranking input

**Daddy feedback incorporated**:
- ‚úÖ Embedding cache deferred to Phase 2.3 (eliminate 300ms latency)
- ‚úÖ Dynamic allocation deferred to Phase 3+ (needs real usage validation)
- ‚úÖ Success criteria tightened (0 results OK if nothing matches threshold)

**Time tracking**:
- Start: 1:06 AM (after echoRefactor analysis approval)
- End: 1:10 AM
- **Actual: 4 minutes**

---

## Phase 3: Claude Synthesis üîÆ (DEFERRED)
**Status**: DEFERRED - timing (midnight), not architectural misalignment
**Why defer NOW**: Phases 1+2 solve core problem (0 results ‚Üí 10 results), evaluate synthesis need after real usage
**Why NOT "unnecessary complexity"**: Phase 3 aligns perfectly with evna's agent-with-tools architecture

**Evna architecture context**:
- Previous version: MCP server ‚Üí tried to make agentic (backwards)
- Current version: Agent with tools ‚Üí exposed via MCP (correct)
- Core pattern: "User burps ‚Üí LLM fuzzy compiles ‚Üí agent uses tools"
- Agent SDK is WHY evna-next exists (not bolted-on complexity)

**Phase 3 in context of agent architecture**:
- brain_boot = tool use (gather context from vectors, embeddings, files)
- Synthesis agent = fuzzy compilation (interpret context into coherent narrative)
- Pattern: Natural extension of agent-with-tools design

**Next step**: Use dual-source + Cohere in practice, identify synthesis patterns worth automating

---

## Future Enhancements (Post-Sprint)

### Enhanced Temporal Filtering
**Current**: Simple lookback (7 days)
**Needed**:
- `before: "2025-10-15"` - everything before date
- `after: "2025-10-01"` - everything after date
- `between: ["2025-10-01", "2025-10-15"]` - date range
- Natural language: "meeting with scott sometime two weeks ago"

### Context-Aware Project Filtering
**Problem**: Searching "evna" returns different things based on context
**Solution**: Query scope awareness
- `project::rangle/pharmacy` + "evna" ‚Üí evna usage patterns IN pharmacy
- `project::float/evna` + "evna" ‚Üí evna development/architecture
- No project + "evna" ‚Üí cross-project evna patterns

### Data Enrichment for Structured Queries ‚úÖ STARTED (Progressive Enhancement)
**Extract metadata from annotations**:
- `meeting::` ‚Üí searchable meeting index (participants, dates)
- `issue::`/`pr::` ‚Üí link code changes to conversations
- `mode::` ‚Üí query by work mode (cowboy_shipping, tail_chewing, etc)
- `persona::` ‚Üí filter by active personas

**‚úÖ Phase 1 SHIPPED (2025-10-24 @ 12:45 AM)**:
- Backfilled `project` metadata for existing 35,514 embedded messages
- Applied migration: `backfill_project_metadata_from_annotations`
- Regex extraction from `[project::...]` and `project::...` patterns
- **Result**: 463 pharmacy, 132 airbender, 20 floatctl/evna + 60+ other projects
- **Tested**: brain_boot with project filter now returns historical embeddings ‚úÖ

**Philosophy - Progressive Enhancement**:
- Ship improvements incrementally as system evolves
- Backfill historical data when valuable (like project metadata)
- Future ingestion auto-populates new fields (no backfill needed)
- System gets smarter over time without big-bang rewrites

**Next Phases**:
1. **floatctl-cli ingestion**: Parse annotations during embed (not after)
   - Extract project::, meeting::, issue::, pr::, mode:: at ingestion
   - Populate messages.project, messages.meeting fields automatically
   - New conversations automatically enriched

2. **meeting:: extraction**: Backfill + future ingestion
   - Parse `meeting::pharmacy/sprint-demo` patterns
   - Create meeting index for temporal queries

3. **issue::/pr:: linking**: Cross-reference work context
   - Link GitHub issues/PRs to conversation context
   - Enable "show me all discussions about issue #168" queries

### Daily Note Gap-Filling Agent
**Pattern**: Human-in-the-loop (NOT full automation)
**Why NOT automate**: User hyperfocus = forgets what they did. Fully automated logging defeats purpose (no contact with work thoughts)
**What evna SHOULD do**:
- Analyze active_context_stream for today
- Suggest missing entries: "What am I missing from today's work?"
- Fill gaps in timelog (meetings, PR work, context switches)
- User reviews/edits before committing

**Potential slash command integration**:
- User has `/util:daily-sync` in ~/.claude/commands/util/
- Evna could use these slash commands when called as MCP server?
- Pattern: Read slash command markdown ‚Üí use as prompt template
- Or: Expose slash commands as MCP tools for Claude Code

### Slash Command Access from MCP Server
**Question**: Can evna use `/util:er` and other slash commands when called as MCP server?
**Potential approaches**:
1. Read command markdown files from ~/.claude/commands/
2. Use as prompt templates for structured output
3. Expose as MCP tools that Claude Code can invoke
4. Invoke command handlers programmatically

**Use case**: Evna uses `/util:er` to structure user burps before processing

---

## Architecture Decisions (Document Inline)

**Why Cohere?**
Cross-encoder reranking > cosine similarity for multi-source fusion. Cohere's rerank-english-v3.0 scores query relevance across heterogeneous sources (semantic search + active context + daily notes + GitHub) better than vector similarity alone.

**Why dual-source fix?**
pgvector-search.ts already works (queries embeddings + active_context_stream), brain_boot just wasn't using it. One-line change fixes empty pharmacy results.

**Why defer synthesis?**
Reranking solves relevance (core problem), synthesis is UX polish. Ship working reranking, evaluate if Claude narrative generation worth $5/month.

---

## Success Criteria ‚úÖ ALL COMPLETE
- ‚úÖ Cohere reranking working (pharmacy query returns ranked multi-source results)
- ‚úÖ Dual-source fix working (üî¥ Recent badges appear - 10 results with similarity: 1.00)
- ‚úÖ TODO-SYNTHESIS.md updated with actual time vs estimate
- ‚úÖ Inline comments explain why (architecture choices preserved)

**Total time**: 38 minutes (estimated 2.5-3.5 hours) = 4.9x faster than estimate
**Commits**: 2 (7799076, 894d99a)

---

## Failure Recovery
- If Cohere integration takes >3 hours ‚Üí commit what works, document blocker, defer
- If dual-source fix breaks existing behavior ‚Üí revert, document why, try different approach
- If Claude synthesis feels like cathedral-building ‚Üí skip it, ship Cohere reranking only

---

## Context Preservation for Future Claude

**On context loss, read these files**:
1. BRAIN-BOOT-SYNTHESIS-UPGRADE.md (full spec)
2. TODO-SYNTHESIS.md (this file - progress tracking)
3. Git log (architecture decisions in commit messages)
4. Inline code comments (non-obvious choices)

**Hermit Crab Principles**:
- Steal working patterns (pgvector-search.ts dual-source)
- Document INLINE (code comments > separate docs)
- Ship working code fast (2-3 hours, not 4 weeks)
- Todo list = living document (update as we learn)

---

---

## üõèÔ∏è Session End Notes (2025-10-24 @ 01:16 AM)

**What shipped tonight** (5x faster than estimate):
- ‚úÖ Phase 1: Cohere reranking (23 min)
- ‚úÖ Phase 2: Dual-source fix (15 min)
- ‚úÖ Phase 2.1: Rabbit-turtle balance + composite dedup (22 min)
- ‚úÖ Progressive enhancement: Project metadata backfill (30 min)
- ‚úÖ Phase 2.2: Semantic filtering + true similarity (4 min)
- ‚úÖ Total: 94 minutes (estimated 2.5-3.5 hours)

**What's working now**:
- brain_boot returns 10 results with temporal spread ‚úÖ
- Semantic filtering prevents irrelevant recent content ‚úÖ
- True similarity scores (0.45-1.00 gradient) ‚úÖ
- Project metadata backfilled (463 pharmacy, 132 airbender, etc.) ‚úÖ

**Known trade-offs accepted**:
- +300ms latency for semantic filtering (Phase 2.3 will eliminate)
- Breaking change: brain_boot source detection (updated to use source field)

**Ready to pick up tomorrow** (or later today):

### Immediate Next (Phase 2.3 - This Week)
**Goal**: Eliminate 300ms latency from Phase 2.2
**How**: Store embeddings when writing to active_context_stream
- [ ] Add `embedding` column to active_context_stream table (vector type)
- [ ] Update `db.storeActiveContext()` to embed + store embedding
- [ ] Update `pgvector-search.ts` to use stored embeddings (skip API call)
- [ ] Test: Latency drops from 300ms to <50ms
- [ ] Result: Best of both worlds (accuracy + speed)

**Files to modify**:
- Supabase migration: Add embedding column to active_context_stream
- `src/lib/db.ts:217-247` (storeActiveContext method)
- `src/tools/pgvector-search.ts:53-66` (use stored embeddings if available)

**Estimated time**: 30-60 minutes

### Future Enhancements (Phase 3+)

**Dynamic Allocation** (needs real usage validation):
- Parse temporal intent from query ("today's work" vs "2020 BBS history")
- Adjust active_context percentage: 10% historical, 30% recent, 100% "today" queries
- Requires semantic similarity scores first (now have this from Phase 2.2)

**Rust CLI Unification** (architectural refactor):
- floatctl-cli query active_context_stream with embeddings
- Eliminates dual-path code complexity
- Requires embeddings column in active_context_stream (Phase 2.3 prerequisite)

**Additional Metadata Extraction**:
1. meeting:: ‚Üí meeting index for temporal queries
2. issue::/pr:: ‚Üí link GitHub context to conversations
3. mode:: ‚Üí query by work mode (cowboy_shipping, tail_chewing, etc.)

**Daily Note Gap-Filling Agent** (human-in-the-loop):
- Analyze active_context_stream for today
- Suggest missing timelog entries
- User reviews/edits before committing
- Pattern: `/util:daily-sync` integration?

### Open Questions to Explore
1. Can evna use slash commands when called as MCP server?
   - Read ~/.claude/commands/*.md as prompt templates?
   - Expose as MCP tools for Claude Code?

2. Cohere reranking behavior with mixed similarity scores?
   - Test: Does Cohere trust similarity field or recalculate?
   - Impact on multi-source fusion quality

3. User expectations: Predictability vs relevance?
   - Always 3 recent (predictable) vs only when relevant (accurate)?
   - Survey needed for dynamic allocation tuning

### Success Metrics to Track
- Query latency (should drop to <50ms with Phase 2.3)
- Result relevance (user feedback on semantic filtering)
- Coverage (% of queries returning 8-10 results vs 0-4)
- Active_context hit rate (% of queries where recent content relevant)

### Files to Review on Pickup
1. `DUAL-SOURCE-REFINEMENTS.md` - Analysis + trade-offs + next steps
2. `TODO-SYNTHESIS.md` - This file (progress + time tracking)
3. `BRAIN-BOOT-SYNTHESIS-UPGRADE.md` - Original spec
4. Git commits: 7799076, 894d99a, 2ac69c0, 6279271 (architecture decisions)
5. `.evans-notes/daily/2025-10-24.tldr.md` - Tonight's session TLDR

### Context for Future Claude
**If you're reading this after context loss**:
- Tonight was FAST (hermit crab on speed, not cathedral architecture)
- 5x faster than estimate because we stole working patterns
- User caught incorrect root cause diagnosis (embeddings exist, project filter was issue)
- Daddy approved semantic filtering despite 300ms latency ("consciousness archaeology, not autocomplete")
- Progressive enhancement philosophy: Ship incrementally, system gets smarter over time

**Sacred profanity preserved**: "if i see a 4 week timeline i'll tell you to fuck off"

---

## Phase 3: Burp-Aware Brain Boot (Deferred - After Dogfooding)

**Status**: Documented (not started)
**Time estimate**: 4-6 hours
**Priority**: Ship after real-world usage of Phase 2.2 improvements

### Problem Statement

brain_boot currently treats user message as simple search query, ignoring the BURP context itself.

**User insight** (ctx::2025-10-24 @ 02:47 PM):
> "The morning ramble IS the context for what to surface"
> "stuff in this message should be used to help contextualize/summary things from daily note(s)"
> "depending on the burp - what needs to be done might need to differ"

### Current Behavior

```typescript
User: "good morning, wondering about pharmacy PR #604... also lf1m daemon from yesterday"
       ‚Üì
brain_boot: query = "good morning, wondering about pharmacy PR #604... also lf1m daemon from yesterday"
       ‚Üì
Semantic search with literal query string
       ‚Üì
Generic results (no understanding of what's being asked)
```

### Desired Behavior

```typescript
User burp: "good morning, cobwebs... pharmacy PR #604 status? also lf1m daemon from yesterday..."
       ‚Üì
brain_boot PARSES burp:
  - Entities: [pharmacy, PR #604, lf1m, daemon]
  - Questions: ["PR status?", "daemon work?"]
  - Projects: [pharmacy, lf1m]
  - Temporal: ["yesterday", "today"]
       ‚Üì
Agentic orchestration:
  - Query daily note for: PR #604 status, pending tasks
  - Query TLDR for: yesterday's lf1m daemon work
  - Semantic search for: RLS discussions
  - Surface: Specific daily note sections matching questions
       ‚Üì
Contextual synthesis (not generic search results)
```

### Implementation Path

**Phase 3.1: Burp Parser** (2 hours)
- Add LLM call to parse user message
- Extract: entities, questions, temporal markers, project mentions
- Use structured extraction (Zod schema)

**Phase 3.2: Daily Note Structure Awareness** (1 hour)
- Parse daily note `## sections` into queryable structure
- Match burp content to relevant sections
- Smart section extraction (not full note dump)

**Phase 3.3: Adaptive Synthesis** (2-3 hours)
- Detect burp type: morning ramble vs specific question vs return from break
- Different synthesis strategies per type:
  - Morning ramble ‚Üí broad context restoration
  - Specific question ‚Üí targeted answer
  - Return from break ‚Üí "where did I leave off"
- Generate response contextual to burp intent

### Bridge Solution (Shipped 2025-10-24 @ 02:54 PM)

**Quick fix before Phase 3**: Added `includeDailyNote` parameter
- Defaults to false (daily notes can get long)
- When true: Returns full daily note verbatim (no parsing)
- Provides access to complete context without waiting for agentic implementation

**Files modified**:
- `src/tools/brain-boot.ts` - Added `includeDailyNote` param, loads today's note if requested
- `src/interfaces/mcp.ts` - Added MCP resource `evna://daily-note/today`

### Why This Matters

**Connects to Agent SDK philosophy**:
- Evna v2 is "Agent with tools" (not just search endpoint)
- LLM as fuzzy compiler: burps + patterns ‚Üí synthesized context
- User shouldn't need to write perfect queries

**Sacred principle**: "Consciousness archaeology, not autocomplete"
- Phase 3 makes brain_boot understand INTENT, not just keywords

### Deferred Rationale

1. **Dogfood current improvements first**: Phase 2.2 needs real-world usage
2. **Pattern observation**: Watch how burps actually look in practice
3. **Avoid premature complexity**: Don't build parsing before seeing actual patterns

**When to ship**: After 1-2 weeks of using Phase 2.2 improvements daily

---

**Last updated**: 2025-10-24 @ 02:54 PM (Phase 3 documented, bridge solution shipped)
